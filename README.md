# Documentation of Submission of Julian and Ali

```bash
project_root/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/
â”‚   â”‚   â”œâ”€â”€ scene_001.json
â”‚   â”‚   â””â”€â”€ scene_002.json
â”‚   â”œâ”€â”€ output/
â”‚   â”‚   â”œâ”€â”€ scene_001.json        # Ground truth labels
â”‚   â”‚   â””â”€â”€ scene_002.json
â”‚   â””â”€â”€ predictions/
â”‚       â”œâ”€â”€ scene_001.json        # Generated by your dummy model or real model
â”‚       â””â”€â”€ scene_002.json
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ dummy_model.py            # DummyModel class
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ box_utils.py              # create_3d_box, compute_3d_iou, etc.
â”‚   â””â”€â”€ json_utils.py             # load_json, save_json
â”‚
â”œâ”€â”€ main.py                       # Entry point for prediction + evaluation + visualization
â”‚
â”œâ”€â”€ evaluate.py                   # Batch evaluation script (e.g., average IOU over all scenes)
â”‚
â””â”€â”€ README.md                     # How to run and structure scenes
```


# Fusion_event


## ğŸ” Problem

Autonomous vehicles rely on sensors like LiDAR and cameras to perceive their environment. Each sensor has strengths and limitations:

ğŸ”¹ LiDAR provides accurate 3D data but lacks color information.

ğŸ”¹ Cameras capture rich visuals but are sensitive to lighting conditions.


Individually, these sensors can be noisy or miss key details due to occlusions from other road agents. However, by fusing data from multiple sensors and vehicles, we can create a more reliable, comprehensive view of the scene, improving safety and awareness.

## ğŸš¦ Scenario

Two self-driving cars are approaching an intersection, each equipped with:


âœ… 3D LiDAR

âœ… Camera

The environment includes vehicles, pedestrians, and cyclists, some of whom may block each vehicleâ€™s view. By communicating and sharing sensor data, the vehicles can collaborate to overcome occlusions and enhance situational understanding.

![scene](/images/scene.png)

## Dataset Description

The [/data/input](/data/input) folder contains a JSON metadata file for each scene. Each JSON file contains the following information

| Field | Description | Unit |
| --- | --- | --- |
| CarA_Camera | Path to the image captured by the first vehicle's camera |  |
| CarA_Lidar | Path to the point cloud captured by the first vehicle's Lidar|  |
| CarA_Location | The location of the center of the first vehicle (x, y) | meters |
| CarA_Rotation | The rotation of the first vehicle | degree |
| CarA_Dimension | The dimensions of the first vehicle (Length, Width, Height) | m |
| CarB_Camera | Path to the image captured by the second vehicle's camera |  |
| CarB_Lidar | Path to the point cloud captured by the second vehicle's Lidar|  |
| CarB_Location | The location of the center of the second vehicle (x, y) | meters |
| CarB_Rotation | The rotation of the second vehicle | degree |
| CarB_Dimension | The dimensions of the second vehicle (Length, Width, Height) | meters |

The [/data/output](/data/output) folder contains JSON files of all other road agents in the scene. Each JSON file contains an array of 

| Field | Description | Unit |
| --- | --- | --- |
| Object | the type of road agents (Car/Pedestrian) |  |
| Location | The location of the center of the road agent (x, y) | meters |
| Rotation | The rotation of the road agent | degree |
| Dimension | The dimensions of the road agent (Length, Width, Height) | meters |

  ## ğŸ¯ Goal

Process the raw camera and LiDAR data from both vehicles to:


ğŸ”¹ Generate individual object detection outputs for each car.

ğŸ”¹ Fuse the data to build a shared perception of the scene.

ğŸ”¹ Enhance visibility by addressing sensor occlusions and inconsistencies.

ğŸ”¹ Output a visual representation showing detected agents from both perspectives.
